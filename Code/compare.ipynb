{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import traceback\n",
    "from joblib import load\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    BaggingClassifier, \n",
    "    IsolationForest\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    RidgeClassifier,\n",
    "    RidgeClassifierCV,\n",
    "    SGDClassifier,\n",
    "    Perceptron,\n",
    "    PassiveAggressiveClassifier,\n",
    "    LogisticRegressionCV,\n",
    "    BayesianRidge,\n",
    "    PassiveAggressiveRegressor,\n",
    "    HuberRegressor,\n",
    "    RANSACRegressor,\n",
    "    TheilSenRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC, OneClassSVM\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier,\n",
    "    RadiusNeighborsClassifier,\n",
    "    NearestCentroid,\n",
    "    LocalOutlierFactor\n",
    ")\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    BernoulliNB,\n",
    "    CategoricalNB\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:TensorFlow/Keras not available\n",
      "WARNING:__main__:FastAI not available\n",
      "WARNING:__main__:River not available\n",
      "WARNING:__main__:Sktime not available\n",
      "WARNING:__main__:AutoGluon not available\n",
      "WARNING:__main__:Hyperopt not available\n"
     ]
    }
   ],
   "source": [
    "LIBRARY_STATUS = {\n",
    "    'xgboost': False,\n",
    "    'lightgbm': False,\n",
    "    'catboost': False,\n",
    "    'keras': False,\n",
    "    'ngboost': False,\n",
    "    'river': False,\n",
    "    'sktime': False,\n",
    "    'torch': False,\n",
    "    'tensorflow': False,\n",
    "    'thundersvm': False,\n",
    "    'cuml': False,\n",
    "    'fastai': False,\n",
    "    'autogluon': False,\n",
    "    'hyperopt': False\n",
    "}\n",
    "\n",
    "# Standard boosting libraries\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    LIBRARY_STATUS['xgboost'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIBRARY_STATUS['lightgbm'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    LIBRARY_STATUS['catboost'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"CatBoost not available\")\n",
    "\n",
    "# Deep Learning frameworks\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, GRU\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "    LIBRARY_STATUS['keras'] = True\n",
    "    LIBRARY_STATUS['tensorflow'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"TensorFlow/Keras not available\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    LIBRARY_STATUS['torch'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    from fastai.tabular.all import *\n",
    "    LIBRARY_STATUS['fastai'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"FastAI not available\")\n",
    "\n",
    "# Advanced boosting and online learning\n",
    "try:\n",
    "    from ngboost import NGBClassifier\n",
    "    LIBRARY_STATUS['ngboost'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"NGBoost not available\")\n",
    "\n",
    "try:\n",
    "    import river\n",
    "    from river import linear_model, tree, ensemble\n",
    "    from river.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "    from river.naive_bayes import NaiveBayes\n",
    "    LIBRARY_STATUS['river'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"River not available\")\n",
    "\n",
    "# Time series specific\n",
    "try:\n",
    "    from sktime.classification.sklearn import SklearnClassifier\n",
    "    from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "    LIBRARY_STATUS['sktime'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"Sktime not available\")\n",
    "\n",
    "# AutoML and optimization\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    LIBRARY_STATUS['autogluon'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"AutoGluon not available\")\n",
    "\n",
    "try:\n",
    "    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "    LIBRARY_STATUS['hyperopt'] = True\n",
    "except ImportError:\n",
    "    logger.warning(\"Hyperopt not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "Pandas Version: 2.2.3\n",
      "TensorFlow Version: 2.18.0\n",
      "Keras Version: 3.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_dim=input_dim, kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, epochs=100, lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _create_model(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(self.device)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        y = torch.FloatTensor(y).unsqueeze(1).to(self.device)\n",
    "\n",
    "        self.model = self._create_model()\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            predictions = self.model(X)\n",
    "        return (predictions.cpu().numpy() > 0.5).astype(int).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     data = np.load('../Data/#1/processed_data.npz')\n",
    "#     x_tr_resample = data['x_tr_resample']\n",
    "#     y_tr_resample = data['y_tr_resample']\n",
    "#     X_test = data['X_test']\n",
    "#     y_test = data['y_test']\n",
    "#     X_train = data['X_train']\n",
    "\n",
    "#     norm = load('../Data/#1/power_transformer.joblib')\n",
    "#     norm_train_feature = norm.fit_transform(X_train)\n",
    "#     norm_test_feature = norm.transform(X_test)\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Error loading data: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "x_tr_resample = pd.read_csv('../Data/sequential/n=7/X_train_smote.csv')\n",
    "y_tr_resample = pd.read_csv('../Data/sequential/n=7/y_train_smote.csv')\n",
    "X_test = pd.read_csv('../Data/sequential/n=7/X_test.csv')\n",
    "y_test = pd.read_csv('../Data/sequential/n=7/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    # Scikit-learn Base Classifiers\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Extra Tree': ExtraTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingClassifier(random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Logistic Regression CV': LogisticRegressionCV(random_state=42),\n",
    "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
    "    'Ridge Classifier CV': RidgeClassifierCV(),\n",
    "    'SGD Classifier': SGDClassifier(random_state=42),\n",
    "    'Bayesian Ridge': BayesianRidge(),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'Passive Aggressive': PassiveAggressiveClassifier(random_state=42),\n",
    "    'SVM (RBF Kernel)': SVC(random_state=42),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42),\n",
    "    'SVM (Polynomial Kernel)': SVC(kernel='poly', random_state=42),\n",
    "    'Linear SVM': LinearSVC(random_state=42),\n",
    "    'Nu SVM': NuSVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Radius Nearest Neighbors': RadiusNeighborsClassifier(radius=1.0, outlier_label='most_frequent'),\n",
    "    'Nearest Centroid': NearestCentroid(),\n",
    "    'One-Class SVM': OneClassSVM(),\n",
    "    'Calibrated Classifier': CalibratedClassifierCV(cv=5),\n",
    "    'Bagging Classifier': BaggingClassifier(random_state=42),\n",
    "    'Isolation Forest': IsolationForest(random_state=42),\n",
    "    'Local Outlier Factor': LocalOutlierFactor(n_neighbors=20, novelty=True),\n",
    "\n",
    "    # Naive Bayes Classifiers\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Complement Naive Bayes': ComplementNB(),\n",
    "    'Bernoulli Naive Bayes': BernoulliNB(),\n",
    "    'Categorical Naive Bayes': CategoricalNB(),\n",
    "\n",
    "    # Advanced Classifiers\n",
    "    'Multi-Layer Perceptron': MLPClassifier(random_state=42, max_iter=1000),\n",
    "    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),\n",
    "    'Label Spreading': LabelSpreading(),\n",
    "    'Label Propagation': LabelPropagation(),\n",
    "    'Gaussian Process': GaussianProcessClassifier(random_state=42),\n",
    "\n",
    "    'Dummy Classifier': DummyClassifier(strategy='most_frequent'),\n",
    "    'Gaussian Process Regressor': GaussianProcessRegressor(),\n",
    "    'Passive Aggressive Regressor': PassiveAggressiveRegressor(),\n",
    "    'Huber Regressor': HuberRegressor(),\n",
    "    'RANSAC Regressor': RANSACRegressor(),\n",
    "    'Theil Sen Regressor': TheilSenRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIBRARY_STATUS['xgboost']:\n",
    "    classifiers.update({\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'XGBoost DART': XGBClassifier(random_state=42, booster='dart', use_label_encoder=False),\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS['lightgbm']:\n",
    "    classifiers.update({\n",
    "        'LightGBM': LGBMClassifier(random_state=42),\n",
    "        'LightGBM DART': LGBMClassifier(boosting_type='dart', random_state=42),\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS['catboost']:\n",
    "    classifiers.update({\n",
    "        'CatBoost': CatBoostClassifier(random_state=42, verbose=0),\n",
    "        'CatBoost L2': CatBoostClassifier(random_state=42, loss_function='MultiClass', verbose=0),\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS['keras']:\n",
    "    classifiers.update({\n",
    "        'Keras Neural Network': KerasClassifier(\n",
    "            build_fn=lambda: create_keras_model(x_tr_resample.shape[1]),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS['ngboost']:\n",
    "    classifiers.update({\n",
    "        'NGBoost Classifier': NGBClassifier(random_state=42)\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS['torch']:\n",
    "    classifiers.update({\n",
    "        'PyTorch Neural Network': PyTorchClassifier(input_dim=x_tr_resample.shape[1])\n",
    "    })\n",
    "\n",
    "if LIBRARY_STATUS.get('river', False): \n",
    "    classifiers.update({\n",
    "        'Passive Aggressive River': PassiveAggressiveClassifier(),\n",
    "        'River Logistic Regression': LogisticRegression(),\n",
    "        'River Naive Bayes': NaiveBayes(),\n",
    "    })\n",
    "\n",
    "# if LIBRARY_STATUS['sktime']:\n",
    "#     classifiers.update({\n",
    "#         'Time Series Forest': TimeSeriesForestClassifier(random_state=42),\n",
    "#         'Sktime Sklearn Wrapper': SklearnClassifier(estimator=RandomForestClassifier(random_state=42)),\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, x_tr_resample, y_tr_resample, X_test, y_test):\n",
    "    try:\n",
    "        # Special handling for OneClassSVM and unsupervised models\n",
    "        if name == 'One-Class SVM':\n",
    "            model.fit(x_tr_resample)\n",
    "            preds = model.predict(X_test)\n",
    "            preds = [1 if p == 1 else 0 for p in preds]\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "        else:\n",
    "            # Model Training and Prediction\n",
    "            model.fit(x_tr_resample, y_tr_resample)\n",
    "            preds = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "        \n",
    "        # Print the model name and accuracy result\n",
    "        print(f\"{name} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        return accuracy\n",
    "    except Exception as e:\n",
    "        # Print model name and error message if exception occurs\n",
    "        print(f\"{name} - Error: {str(e)}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(name, model, x_tr_resample, y_tr_resample):\n",
    "    try:\n",
    "        # Cross-validation using StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, x_tr_resample, y_tr_resample, cv=skf, scoring='accuracy')\n",
    "        \n",
    "        return cv_scores.mean(), cv_scores.std()\n",
    "    except Exception as cv_error:\n",
    "        logger.warning(f\"Cross-validation failed for {name}: {cv_error}\")\n",
    "        return 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 98.22%\n",
      "Extra Tree - Accuracy: 98.43%\n",
      "Random Forest - Accuracy: 99.14%\n",
      "Gradient Boosting - Accuracy: 98.83%\n",
      "Hist Gradient Boosting - Accuracy: 98.88%\n",
      "Extra Trees - Accuracy: 99.14%\n",
      "AdaBoost - Accuracy: 98.48%\n",
      "Logistic Regression - Error: 'LogisticRegression' object has no attribute 'fit'\n",
      "Logistic Regression CV - Accuracy: 97.11%\n",
      "Ridge Classifier - Accuracy: 96.60%\n",
      "Ridge Classifier CV - Accuracy: 96.60%\n",
      "SGD Classifier - Accuracy: 97.61%\n",
      "Bayesian Ridge - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "Perceptron - Accuracy: 95.53%\n",
      "Passive Aggressive - Accuracy: 88.12%\n",
      "SVM (RBF Kernel) - Accuracy: 98.53%\n",
      "SVM (Linear Kernel) - Accuracy: 96.65%\n",
      "SVM (Polynomial Kernel) - Accuracy: 95.28%\n",
      "Linear SVM - Accuracy: 97.00%\n",
      "Nu SVM - Accuracy: 94.82%\n",
      "K-Nearest Neighbors - Accuracy: 97.92%\n",
      "Radius Nearest Neighbors - Accuracy: 97.61%\n",
      "Nearest Centroid - Accuracy: 73.74%\n",
      "One-Class SVM - Accuracy: 45.81%\n",
      "Calibrated Classifier - Accuracy: 96.95%\n",
      "Bagging Classifier - Accuracy: 98.98%\n",
      "Isolation Forest - Accuracy: 17.78%\n",
      "Local Outlier Factor - Accuracy: 18.69%\n",
      "Gaussian Naive Bayes - Accuracy: 95.89%\n",
      "Multinomial Naive Bayes - Accuracy: 92.53%\n",
      "Complement Naive Bayes - Accuracy: 92.53%\n",
      "Bernoulli Naive Bayes - Accuracy: 78.31%\n",
      "Categorical Naive Bayes - Accuracy: 90.40%\n",
      "Multi-Layer Perceptron - Accuracy: 98.63%\n",
      "Linear Discriminant Analysis - Accuracy: 96.60%\n",
      "Quadratic Discriminant Analysis - Accuracy: 97.77%\n",
      "Label Spreading - Accuracy: 98.22%\n",
      "Label Propagation - Accuracy: 98.22%\n",
      "Gaussian Process - Accuracy: 98.68%\n",
      "Dummy Classifier - Accuracy: 78.31%\n",
      "Gaussian Process Regressor - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "Passive Aggressive Regressor - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "Huber Regressor - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "RANSAC Regressor - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "Theil Sen Regressor - Error: Classification metrics can't handle a mix of binary and continuous targets\n",
      "XGBoost - Accuracy: 98.88%\n",
      "XGBoost DART - Accuracy: 98.88%\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6120, number of negative: 6120\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1411\n",
      "[LightGBM] [Info] Number of data points in the train set: 12240, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM - Accuracy: 98.93%\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6120, number of negative: 6120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1411\n",
      "[LightGBM] [Info] Number of data points in the train set: 12240, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM DART - Accuracy: 98.88%\n",
      "CatBoost - Accuracy: 99.29%\n",
      "CatBoost L2 - Accuracy: 99.19%\n",
      "NGBoost Classifier - Error: arrays used as indices must be of integer (or boolean) type\n",
      "PyTorch Neural Network - Error: could not determine the shape of object type 'DataFrame'\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    results[name] = evaluate_model(name, classifier, x_tr_resample, y_tr_resample, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ranking Model Berdasarkan Akurasi Uji ---\n",
      "1. CatBoost: 99.29%\n",
      "2. CatBoost L2: 99.19%\n",
      "3. Random Forest: 99.14%\n",
      "4. Extra Trees: 99.14%\n",
      "5. Bagging Classifier: 98.98%\n",
      "6. LightGBM: 98.93%\n",
      "7. Hist Gradient Boosting: 98.88%\n",
      "8. XGBoost: 98.88%\n",
      "9. XGBoost DART: 98.88%\n",
      "10. LightGBM DART: 98.88%\n",
      "11. Gradient Boosting: 98.83%\n",
      "12. Gaussian Process: 98.68%\n",
      "13. Multi-Layer Perceptron: 98.63%\n",
      "14. SVM (RBF Kernel): 98.53%\n",
      "15. AdaBoost: 98.48%\n",
      "16. Extra Tree: 98.43%\n",
      "17. Decision Tree: 98.22%\n",
      "18. Label Spreading: 98.22%\n",
      "19. Label Propagation: 98.22%\n",
      "20. K-Nearest Neighbors: 97.92%\n",
      "21. Quadratic Discriminant Analysis: 97.77%\n",
      "22. SGD Classifier: 97.61%\n",
      "23. Radius Nearest Neighbors: 97.61%\n",
      "24. Logistic Regression CV: 97.11%\n",
      "25. Linear SVM: 97.00%\n",
      "26. Calibrated Classifier: 96.95%\n",
      "27. SVM (Linear Kernel): 96.65%\n",
      "28. Ridge Classifier: 96.60%\n",
      "29. Ridge Classifier CV: 96.60%\n",
      "30. Linear Discriminant Analysis: 96.60%\n",
      "31. Gaussian Naive Bayes: 95.89%\n",
      "32. Perceptron: 95.53%\n",
      "33. SVM (Polynomial Kernel): 95.28%\n",
      "34. Nu SVM: 94.82%\n",
      "35. Multinomial Naive Bayes: 92.53%\n",
      "36. Complement Naive Bayes: 92.53%\n",
      "37. Categorical Naive Bayes: 90.40%\n",
      "38. Passive Aggressive: 88.12%\n",
      "39. Bernoulli Naive Bayes: 78.31%\n",
      "40. Dummy Classifier: 78.31%\n",
      "41. Nearest Centroid: 73.74%\n",
      "42. One-Class SVM: 45.81%\n",
      "43. Local Outlier Factor: 18.69%\n",
      "44. Isolation Forest: 17.78%\n"
     ]
    }
   ],
   "source": [
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n--- Ranking Model Berdasarkan Akurasi Uji ---\")\n",
    "for rank, (name, accuracy) in enumerate(sorted_results, 1):\n",
    "    if accuracy > 0:\n",
    "        print(f\"{rank}. {name}: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3434\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3538\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3429\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Cross-validation failed for XGBoost: 'super' object has no attribute '__sklearn_tags__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3545\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3542\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Cross-validation failed for XGBoost DART: 'super' object has no attribute '__sklearn_tags__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3434\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3538\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3429\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3545\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 4892, number of negative: 4892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3542\n",
      "[LightGBM] [Info] Number of data points in the train set: 9784, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Cross-validation failed for Multinomial Naive Bayes: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 762, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 888, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1827, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "WARNING:__main__:Cross-validation failed for Complement Naive Bayes: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 762, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1036, in _count\n",
      "    check_non_negative(X, \"ComplementNB (input X)\")\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1827, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to ComplementNB (input X).\n",
      "\n",
      "WARNING:__main__:Cross-validation failed for Categorical Naive Bayes: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1387, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 735, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1461, in _check_X_y\n",
      "    check_non_negative(X, \"CategoricalNB (input X)\")\n",
      "  File \"c:\\Users\\ACER\\anaconda3\\envs\\TA\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1827, in check_non_negative\n",
      "    raise ValueError(f\"Negative values in data passed to {whom}.\")\n",
      "ValueError: Negative values in data passed to CategoricalNB (input X).\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_accuracies = {name: accuracy for name, accuracy in results.items()}\n",
    "cv_results = {}\n",
    "for name, accuracy in sorted_results:\n",
    "    model = classifiers[name]\n",
    "    cv_mean, cv_std = cross_validate_model(name, model, x_tr_resample, y_tr_resample)\n",
    "    cv_results[name] = (cv_mean, cv_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ranking Model Berdasarkan Cross-Validation ---\n",
      "1. CatBoost: 98.70% | (+0.88%)\n",
      "2. Extra Trees: 98.67% | (+1.00%)\n",
      "3. CatBoost L2: 98.66% | (+1.00%)\n",
      "4. Hist Gradient Boosting: 98.52% | (+0.35%)\n",
      "5. LightGBM: 98.50% | (+0.33%)\n",
      "6. Random Forest: 98.48% | (+0.87%)\n",
      "7. Label Spreading: 98.00% | (+1.60%)\n",
      "8. Label Propagation: 97.98% | (+1.59%)\n",
      "9. Multi-Layer Perceptron: 97.95% | (+1.40%)\n",
      "10. LightGBM DART: 97.85% | (+0.34%)\n",
      "11. Bagging Classifier: 97.82% | (+1.02%)\n",
      "12. Gaussian Process: 97.24% | (+1.00%)\n",
      "13. Radius Nearest Neighbors: 97.07% | (+1.19%)\n",
      "14. K-Nearest Neighbors: 97.06% | (+1.63%)\n",
      "15. Gradient Boosting: 96.72% | (+0.33%)\n",
      "16. SVM (RBF Kernel): 96.69% | (+0.24%)\n",
      "17. PyTorch Neural Network: 96.64% | (+0.80%)\n",
      "18. Decision Tree: 96.52% | (+0.18%)\n",
      "19. Extra Tree: 95.99% | (+1.02%)\n",
      "20. SVM (Polynomial Kernel): 95.27% | (+0.61%)\n",
      "21. AdaBoost: 93.71% | (+-0.09%)\n",
      "22. Nu SVM: 89.93% | (+-3.42%)\n",
      "23. SVM (Linear Kernel): 89.55% | (+0.57%)\n",
      "24. Logistic Regression CV: 89.23% | (+0.66%)\n",
      "25. Calibrated Classifier: 89.22% | (+0.64%)\n",
      "26. Logistic Regression: 89.21% | (+0.74%)\n",
      "27. Linear SVM: 89.14% | (+0.82%)\n",
      "28. Linear Discriminant Analysis: 89.01% | (+2.16%)\n",
      "29. Ridge Classifier: 89.00% | (+2.11%)\n",
      "30. Ridge Classifier CV: 89.00% | (+2.11%)\n",
      "31. Quadratic Discriminant Analysis: 88.05% | (+3.38%)\n",
      "32. SGD Classifier: 87.81% | (+2.38%)\n",
      "33. Perceptron: 86.23% | (+12.23%)\n",
      "34. Passive Aggressive: 84.84% | (+-2.56%)\n",
      "35. Bernoulli Naive Bayes: 78.90% | (+-0.99%)\n",
      "36. Gaussian Naive Bayes: 74.41% | (+10.62%)\n",
      "37. Nearest Centroid: 71.43% | (+-0.13%)\n",
      "38. Local Outlier Factor: 46.57% | (+27.53%)\n",
      "39. Isolation Forest: 44.20% | (+24.55%)\n",
      "40. One-Class SVM: 34.60% | (+-31.99%)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ranking Model Berdasarkan Cross-Validation ---\")\n",
    "sorted_cv_results = sorted(cv_results.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "for rank, (name, (cv_mean, cv_std)) in enumerate(sorted_cv_results, 1):\n",
    "    if cv_mean > 0:\n",
    "        previous_accuracy = model_accuracies.get(name, 0)  # Akurasi awal\n",
    "        improvement = (cv_mean - previous_accuracy) * 100  # Perubahan akurasi\n",
    "        print(f\"{rank}. {name}: {cv_mean * 100:.2f}% | (+{improvement:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rendots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
