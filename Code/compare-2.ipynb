{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from ngboost import NGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from river import forest \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    BaggingClassifier, \n",
    "    IsolationForest\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    RidgeClassifier,\n",
    "    RidgeClassifierCV,\n",
    "    SGDClassifier,\n",
    "    Perceptron,\n",
    "    PassiveAggressiveClassifier,\n",
    "    LogisticRegressionCV,\n",
    "    BayesianRidge,\n",
    "    PassiveAggressiveRegressor,\n",
    "    HuberRegressor,\n",
    "    RANSACRegressor,\n",
    "    TheilSenRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC, OneClassSVM\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier,\n",
    "    RadiusNeighborsClassifier,\n",
    "    NearestCentroid,\n",
    "    LocalOutlierFactor\n",
    ")\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB,\n",
    "    MultinomialNB,\n",
    "    ComplementNB,\n",
    "    BernoulliNB,\n",
    "    CategoricalNB\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_neural_network(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layers\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    # Scikit-learn Base Classifiers\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Extra Tree': ExtraTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingClassifier(random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Logistic Regression CV': LogisticRegressionCV(random_state=42),\n",
    "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
    "    'Ridge Classifier CV': RidgeClassifierCV(),\n",
    "    'SGD Classifier': SGDClassifier(random_state=42),\n",
    "    'Bayesian Ridge': BayesianRidge(),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'Passive Aggressive': PassiveAggressiveClassifier(random_state=42),\n",
    "    'SVM (RBF Kernel)': SVC(random_state=42),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42),\n",
    "    'SVM (Polynomial Kernel)': SVC(kernel='poly', random_state=42),\n",
    "    'Linear SVM': LinearSVC(random_state=42),\n",
    "    'Nu SVM': NuSVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Radius Nearest Neighbors': RadiusNeighborsClassifier(radius=1.0, outlier_label='most_frequent'),\n",
    "    'Nearest Centroid': NearestCentroid(),\n",
    "    'One-Class SVM': OneClassSVM(),\n",
    "    'Calibrated Classifier': CalibratedClassifierCV(cv=5),\n",
    "    'Bagging Classifier': BaggingClassifier(random_state=42),\n",
    "    'Isolation Forest': IsolationForest(random_state=42),\n",
    "    'Local Outlier Factor': LocalOutlierFactor(n_neighbors=20, novelty=True),\n",
    "\n",
    "    # Naive Bayes Classifiers\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Complement Naive Bayes': ComplementNB(),\n",
    "    'Bernoulli Naive Bayes': BernoulliNB(),\n",
    "    'Categorical Naive Bayes': CategoricalNB(),\n",
    "\n",
    "    # Advanced Classifiers\n",
    "    'Multi-Layer Perceptron': MLPClassifier(random_state=42, max_iter=1000),\n",
    "    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),\n",
    "    'Label Spreading': LabelSpreading(),\n",
    "    'Label Propagation': LabelPropagation(),\n",
    "    'Gaussian Process': GaussianProcessClassifier(random_state=42),\n",
    "\n",
    "    'Dummy Classifier': DummyClassifier(strategy='most_frequent'),\n",
    "    'Gaussian Process Regressor': GaussianProcessRegressor(),\n",
    "    'Passive Aggressive Regressor': PassiveAggressiveRegressor(),\n",
    "    'Huber Regressor': HuberRegressor(),\n",
    "    'RANSAC Regressor': RANSACRegressor(),\n",
    "    'Theil Sen Regressor': TheilSenRegressor(),\n",
    "}\n",
    "\n",
    "new_classifiers = {\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42),\n",
    "    'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=False),\n",
    "    'NGBoost': NGBClassifier(random_state=42),\n",
    "    'River Random Forest': forest.ARFClassifier(  # Corrected classifier\n",
    "        n_models=10,\n",
    "        seed=42\n",
    "    ),\n",
    "    'Deep Neural Network': 'DNN'\n",
    "}\n",
    "classifiers.update(new_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_neural_network(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Train a deep neural network and make predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training features\n",
    "    y_train : array-like\n",
    "        Training labels\n",
    "    X_test : array-like\n",
    "        Test features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array-like\n",
    "        Predicted labels for test data\n",
    "    \"\"\"\n",
    "    # Create and train DNN\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    model = create_deep_neural_network(X_train.shape[1], num_classes)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Clear Keras session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    return y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers(X_train, y_train, X_test, y_test, use_scaler=False):\n",
    "    \"\"\"\n",
    "    Evaluate all classifiers with optional scaling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_scaler : bool\n",
    "        Whether to use StandardScaler or not\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Prepare the data\n",
    "    if use_scaler:\n",
    "        print(\"Applying StandardScaler transformation...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_prepared = scaler.fit_transform(X_train)\n",
    "        X_test_prepared = scaler.transform(X_test)\n",
    "    else:\n",
    "        print(\"Using data without StandardScaler...\")\n",
    "        X_train_prepared = X_train\n",
    "        X_test_prepared = X_test\n",
    "    \n",
    "    # Evaluate each classifier\n",
    "    for name, clf in classifiers.items():\n",
    "        try:\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            if name == 'Deep Neural Network':\n",
    "                y_pred = train_deep_neural_network(X_train_prepared, \n",
    "                                                 y_train.values.ravel(), \n",
    "                                                 X_test_prepared)\n",
    "            elif name == 'River Random Forest':\n",
    "                # Special handling for River classifier\n",
    "                y_pred = []\n",
    "                model = clf\n",
    "                for i in range(len(X_test_prepared)):\n",
    "                    model.learn_one(X_train_prepared.iloc[i], y_train.iloc[i])\n",
    "                for i in range(len(X_test_prepared)):\n",
    "                    pred = model.predict_one(X_test_prepared.iloc[i])\n",
    "                    y_pred.append(pred)\n",
    "            else:\n",
    "                clf.fit(X_train_prepared, y_train.values.ravel())\n",
    "                y_pred = clf.predict(X_test_prepared)\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            results[name] = accuracy * 100\n",
    "            \n",
    "            print(f\"\\nAccuracy for {name}:\")\n",
    "            print(results[name])\n",
    "            # print(f\"\\nClassification Report for {name}:\")\n",
    "            # print(classification_report(y_test, y_pred))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return dict(sorted(results.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(x_tr_resample, y_tr_resample, X_test, y_test):\n",
    "    # Run without scaler\n",
    "    print(\"\\nRunning models WITHOUT StandardScaler:\")\n",
    "    results_without_scaler = evaluate_classifiers(\n",
    "        x_tr_resample, y_tr_resample, X_test, y_test, \n",
    "        use_scaler=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAccuracy Rankings (WITHOUT StandardScaler):\")\n",
    "    print(\"-\" * 50)\n",
    "    for clf_name, accuracy in results_without_scaler.items():\n",
    "        print(f\"{clf_name:<30} : {accuracy:.2f}%\")\n",
    "    \n",
    "    # Run with scaler\n",
    "    print(\"\\nRunning models WITH StandardScaler:\")\n",
    "    results_with_scaler = evaluate_classifiers(\n",
    "        x_tr_resample, y_tr_resample, X_test, y_test, \n",
    "        use_scaler=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAccuracy Rankings (WITH StandardScaler):\")\n",
    "    print(\"-\" * 50)\n",
    "    for clf_name, accuracy in results_with_scaler.items():\n",
    "        print(f\"{clf_name:<30} : {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_resample = pd.read_csv('../Data/sequential/n=7/X_train_smote.csv')\n",
    "y_tr_resample = pd.read_csv('../Data/sequential/n=7/y_train_smote.csv')\n",
    "X_test = pd.read_csv('../Data/sequential/n=7/X_test.csv')\n",
    "y_test = pd.read_csv('../Data/sequential/n=7/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running models WITHOUT StandardScaler:\n",
      "Using data without StandardScaler...\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Accuracy for Decision Tree:\n",
      "98.22244794311833\n",
      "\n",
      "Training Extra Tree...\n",
      "\n",
      "Accuracy for Extra Tree:\n",
      "98.42559674961909\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Accuracy for Random Forest:\n",
      "99.13661757237176\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Accuracy for Gradient Boosting:\n",
      "98.83189436262062\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "\n",
      "Accuracy for Hist Gradient Boosting:\n",
      "98.88268156424581\n",
      "\n",
      "Training Extra Trees...\n",
      "\n",
      "Accuracy for Extra Trees:\n",
      "99.13661757237176\n",
      "\n",
      "Training AdaBoost...\n",
      "\n",
      "Accuracy for AdaBoost:\n",
      "98.47638395124429\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Accuracy for Logistic Regression:\n",
      "97.15591670898934\n",
      "\n",
      "Training Logistic Regression CV...\n",
      "\n",
      "Accuracy for Logistic Regression CV:\n",
      "97.10512950736414\n",
      "\n",
      "Training Ridge Classifier...\n",
      "\n",
      "Accuracy for Ridge Classifier:\n",
      "96.59725749111225\n",
      "\n",
      "Training Ridge Classifier CV...\n",
      "\n",
      "Accuracy for Ridge Classifier CV:\n",
      "96.59725749111225\n",
      "\n",
      "Training SGD Classifier...\n",
      "\n",
      "Accuracy for SGD Classifier:\n",
      "97.61300152361605\n",
      "\n",
      "Training Bayesian Ridge...\n",
      "Error with Bayesian Ridge: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Perceptron...\n",
      "\n",
      "Accuracy for Perceptron:\n",
      "95.53072625698324\n",
      "\n",
      "Training Passive Aggressive...\n",
      "\n",
      "Accuracy for Passive Aggressive:\n",
      "88.11579481970544\n",
      "\n",
      "Training SVM (RBF Kernel)...\n",
      "\n",
      "Accuracy for SVM (RBF Kernel):\n",
      "98.52717115286947\n",
      "\n",
      "Training SVM (Linear Kernel)...\n",
      "\n",
      "Accuracy for SVM (Linear Kernel):\n",
      "96.64804469273743\n",
      "\n",
      "Training SVM (Polynomial Kernel)...\n",
      "\n",
      "Accuracy for SVM (Polynomial Kernel):\n",
      "95.2767902488573\n",
      "\n",
      "Training Linear SVM...\n",
      "\n",
      "Accuracy for Linear SVM:\n",
      "97.00355510411376\n",
      "\n",
      "Training Nu SVM...\n",
      "\n",
      "Accuracy for Nu SVM:\n",
      "94.81970543423057\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "\n",
      "Accuracy for K-Nearest Neighbors:\n",
      "97.9177247333672\n",
      "\n",
      "Training Radius Nearest Neighbors...\n",
      "\n",
      "Accuracy for Radius Nearest Neighbors:\n",
      "97.61300152361605\n",
      "\n",
      "Training Nearest Centroid...\n",
      "\n",
      "Accuracy for Nearest Centroid:\n",
      "73.74301675977654\n",
      "\n",
      "Training One-Class SVM...\n",
      "\n",
      "Accuracy for One-Class SVM:\n",
      "9.547993905535805\n",
      "\n",
      "Training Calibrated Classifier...\n",
      "\n",
      "Accuracy for Calibrated Classifier:\n",
      "96.95276790248857\n",
      "\n",
      "Training Bagging Classifier...\n",
      "\n",
      "Accuracy for Bagging Classifier:\n",
      "98.9842559674962\n",
      "\n",
      "Training Isolation Forest...\n",
      "\n",
      "Accuracy for Isolation Forest:\n",
      "17.775520568816656\n",
      "\n",
      "Training Local Outlier Factor...\n",
      "\n",
      "Accuracy for Local Outlier Factor:\n",
      "18.689690198070085\n",
      "\n",
      "Training Gaussian Naive Bayes...\n",
      "\n",
      "Accuracy for Gaussian Naive Bayes:\n",
      "95.88623666835957\n",
      "\n",
      "Training Multinomial Naive Bayes...\n",
      "\n",
      "Accuracy for Multinomial Naive Bayes:\n",
      "92.534281361097\n",
      "\n",
      "Training Complement Naive Bayes...\n",
      "\n",
      "Accuracy for Complement Naive Bayes:\n",
      "92.534281361097\n",
      "\n",
      "Training Bernoulli Naive Bayes...\n",
      "\n",
      "Accuracy for Bernoulli Naive Bayes:\n",
      "78.31386490604368\n",
      "\n",
      "Training Categorical Naive Bayes...\n",
      "\n",
      "Accuracy for Categorical Naive Bayes:\n",
      "90.40121889283901\n",
      "\n",
      "Training Multi-Layer Perceptron...\n",
      "\n",
      "Accuracy for Multi-Layer Perceptron:\n",
      "98.62874555611985\n",
      "\n",
      "Training Linear Discriminant Analysis...\n",
      "\n",
      "Accuracy for Linear Discriminant Analysis:\n",
      "96.59725749111225\n",
      "\n",
      "Training Quadratic Discriminant Analysis...\n",
      "\n",
      "Accuracy for Quadratic Discriminant Analysis:\n",
      "97.76536312849163\n",
      "\n",
      "Training Label Spreading...\n",
      "\n",
      "Accuracy for Label Spreading:\n",
      "98.22244794311833\n",
      "\n",
      "Training Label Propagation...\n",
      "\n",
      "Accuracy for Label Propagation:\n",
      "98.22244794311833\n",
      "\n",
      "Training Gaussian Process...\n",
      "\n",
      "Accuracy for Gaussian Process:\n",
      "98.67953275774505\n",
      "\n",
      "Training Dummy Classifier...\n",
      "\n",
      "Accuracy for Dummy Classifier:\n",
      "78.31386490604368\n",
      "\n",
      "Training Gaussian Process Regressor...\n",
      "Error with Gaussian Process Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Passive Aggressive Regressor...\n",
      "Error with Passive Aggressive Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Huber Regressor...\n",
      "Error with Huber Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training RANSAC Regressor...\n",
      "Error with RANSAC Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Theil Sen Regressor...\n",
      "Error with Theil Sen Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Accuracy for XGBoost:\n",
      "98.88268156424581\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 6120, number of negative: 6120\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1411\n",
      "[LightGBM] [Info] Number of data points in the train set: 12240, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Accuracy for LightGBM:\n",
      "98.933468765871\n",
      "\n",
      "Training CatBoost...\n",
      "\n",
      "Accuracy for CatBoost:\n",
      "99.28897917724734\n",
      "\n",
      "Training NGBoost...\n",
      "Error with NGBoost: arrays used as indices must be of integer (or boolean) type\n",
      "\n",
      "Training River Random Forest...\n",
      "Error with River Random Forest: unhashable type: 'Series'\n",
      "\n",
      "Training Deep Neural Network...\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "WARNING:tensorflow:From c:\\Users\\ACER\\anaconda3\\envs\\rendots\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "Accuracy for Deep Neural Network:\n",
      "98.57795835449467\n",
      "\n",
      "Accuracy Rankings (WITHOUT StandardScaler):\n",
      "--------------------------------------------------\n",
      "CatBoost                       : 99.29%\n",
      "Random Forest                  : 99.14%\n",
      "Extra Trees                    : 99.14%\n",
      "Bagging Classifier             : 98.98%\n",
      "LightGBM                       : 98.93%\n",
      "Hist Gradient Boosting         : 98.88%\n",
      "XGBoost                        : 98.88%\n",
      "Gradient Boosting              : 98.83%\n",
      "Gaussian Process               : 98.68%\n",
      "Multi-Layer Perceptron         : 98.63%\n",
      "Deep Neural Network            : 98.58%\n",
      "SVM (RBF Kernel)               : 98.53%\n",
      "AdaBoost                       : 98.48%\n",
      "Extra Tree                     : 98.43%\n",
      "Decision Tree                  : 98.22%\n",
      "Label Spreading                : 98.22%\n",
      "Label Propagation              : 98.22%\n",
      "K-Nearest Neighbors            : 97.92%\n",
      "Quadratic Discriminant Analysis : 97.77%\n",
      "SGD Classifier                 : 97.61%\n",
      "Radius Nearest Neighbors       : 97.61%\n",
      "Logistic Regression            : 97.16%\n",
      "Logistic Regression CV         : 97.11%\n",
      "Linear SVM                     : 97.00%\n",
      "Calibrated Classifier          : 96.95%\n",
      "SVM (Linear Kernel)            : 96.65%\n",
      "Ridge Classifier               : 96.60%\n",
      "Ridge Classifier CV            : 96.60%\n",
      "Linear Discriminant Analysis   : 96.60%\n",
      "Gaussian Naive Bayes           : 95.89%\n",
      "Perceptron                     : 95.53%\n",
      "SVM (Polynomial Kernel)        : 95.28%\n",
      "Nu SVM                         : 94.82%\n",
      "Multinomial Naive Bayes        : 92.53%\n",
      "Complement Naive Bayes         : 92.53%\n",
      "Categorical Naive Bayes        : 90.40%\n",
      "Passive Aggressive             : 88.12%\n",
      "Bernoulli Naive Bayes          : 78.31%\n",
      "Dummy Classifier               : 78.31%\n",
      "Nearest Centroid               : 73.74%\n",
      "Local Outlier Factor           : 18.69%\n",
      "Isolation Forest               : 17.78%\n",
      "One-Class SVM                  : 9.55%\n",
      "\n",
      "Running models WITH StandardScaler:\n",
      "Applying StandardScaler transformation...\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Accuracy for Decision Tree:\n",
      "98.22244794311833\n",
      "\n",
      "Training Extra Tree...\n",
      "\n",
      "Accuracy for Extra Tree:\n",
      "98.42559674961909\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Accuracy for Random Forest:\n",
      "99.13661757237176\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Accuracy for Gradient Boosting:\n",
      "98.83189436262062\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "\n",
      "Accuracy for Hist Gradient Boosting:\n",
      "98.88268156424581\n",
      "\n",
      "Training Extra Trees...\n",
      "\n",
      "Accuracy for Extra Trees:\n",
      "99.13661757237176\n",
      "\n",
      "Training AdaBoost...\n",
      "\n",
      "Accuracy for AdaBoost:\n",
      "98.47638395124429\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Accuracy for Logistic Regression:\n",
      "97.10512950736414\n",
      "\n",
      "Training Logistic Regression CV...\n",
      "\n",
      "Accuracy for Logistic Regression CV:\n",
      "97.10512950736414\n",
      "\n",
      "Training Ridge Classifier...\n",
      "\n",
      "Accuracy for Ridge Classifier:\n",
      "96.59725749111225\n",
      "\n",
      "Training Ridge Classifier CV...\n",
      "\n",
      "Accuracy for Ridge Classifier CV:\n",
      "96.59725749111225\n",
      "\n",
      "Training SGD Classifier...\n",
      "\n",
      "Accuracy for SGD Classifier:\n",
      "96.49568308786186\n",
      "\n",
      "Training Bayesian Ridge...\n",
      "Error with Bayesian Ridge: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Perceptron...\n",
      "\n",
      "Accuracy for Perceptron:\n",
      "90.19807008633825\n",
      "\n",
      "Training Passive Aggressive...\n",
      "\n",
      "Accuracy for Passive Aggressive:\n",
      "91.16302691721685\n",
      "\n",
      "Training SVM (RBF Kernel)...\n",
      "\n",
      "Accuracy for SVM (RBF Kernel):\n",
      "98.78110716099543\n",
      "\n",
      "Training SVM (Linear Kernel)...\n",
      "\n",
      "Accuracy for SVM (Linear Kernel):\n",
      "96.64804469273743\n",
      "\n",
      "Training SVM (Polynomial Kernel)...\n",
      "\n",
      "Accuracy for SVM (Polynomial Kernel):\n",
      "98.88268156424581\n",
      "\n",
      "Training Linear SVM...\n",
      "\n",
      "Accuracy for Linear SVM:\n",
      "97.00355510411376\n",
      "\n",
      "Training Nu SVM...\n",
      "\n",
      "Accuracy for Nu SVM:\n",
      "94.76891823260539\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "\n",
      "Accuracy for K-Nearest Neighbors:\n",
      "98.62874555611985\n",
      "\n",
      "Training Radius Nearest Neighbors...\n",
      "\n",
      "Accuracy for Radius Nearest Neighbors:\n",
      "98.9842559674962\n",
      "\n",
      "Training Nearest Centroid...\n",
      "\n",
      "Accuracy for Nearest Centroid:\n",
      "92.28034535297105\n",
      "\n",
      "Training One-Class SVM...\n",
      "\n",
      "Accuracy for One-Class SVM:\n",
      "7.465718638902996\n",
      "\n",
      "Training Calibrated Classifier...\n",
      "\n",
      "Accuracy for Calibrated Classifier:\n",
      "96.95276790248857\n",
      "\n",
      "Training Bagging Classifier...\n",
      "\n",
      "Accuracy for Bagging Classifier:\n",
      "98.9842559674962\n",
      "\n",
      "Training Isolation Forest...\n",
      "\n",
      "Accuracy for Isolation Forest:\n",
      "17.775520568816656\n",
      "\n",
      "Training Local Outlier Factor...\n",
      "\n",
      "Accuracy for Local Outlier Factor:\n",
      "18.740477399695276\n",
      "\n",
      "Training Gaussian Naive Bayes...\n",
      "\n",
      "Accuracy for Gaussian Naive Bayes:\n",
      "95.88623666835957\n",
      "\n",
      "Training Multinomial Naive Bayes...\n",
      "Error with Multinomial Naive Bayes: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "Training Complement Naive Bayes...\n",
      "Error with Complement Naive Bayes: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "Training Bernoulli Naive Bayes...\n",
      "\n",
      "Accuracy for Bernoulli Naive Bayes:\n",
      "86.33824276282377\n",
      "\n",
      "Training Categorical Naive Bayes...\n",
      "Error with Categorical Naive Bayes: Negative values in data passed to CategoricalNB (input X)\n",
      "\n",
      "Training Multi-Layer Perceptron...\n",
      "\n",
      "Accuracy for Multi-Layer Perceptron:\n",
      "98.933468765871\n",
      "\n",
      "Training Linear Discriminant Analysis...\n",
      "\n",
      "Accuracy for Linear Discriminant Analysis:\n",
      "96.59725749111225\n",
      "\n",
      "Training Quadratic Discriminant Analysis...\n",
      "\n",
      "Accuracy for Quadratic Discriminant Analysis:\n",
      "97.76536312849163\n",
      "\n",
      "Training Label Spreading...\n",
      "\n",
      "Accuracy for Label Spreading:\n",
      "98.78110716099543\n",
      "\n",
      "Training Label Propagation...\n",
      "\n",
      "Accuracy for Label Propagation:\n",
      "98.83189436262062\n",
      "\n",
      "Training Gaussian Process...\n",
      "\n",
      "Accuracy for Gaussian Process:\n",
      "98.88268156424581\n",
      "\n",
      "Training Dummy Classifier...\n",
      "\n",
      "Accuracy for Dummy Classifier:\n",
      "78.31386490604368\n",
      "\n",
      "Training Gaussian Process Regressor...\n",
      "Error with Gaussian Process Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Passive Aggressive Regressor...\n",
      "Error with Passive Aggressive Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Huber Regressor...\n",
      "Error with Huber Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training RANSAC Regressor...\n",
      "Error with RANSAC Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training Theil Sen Regressor...\n",
      "Error with Theil Sen Regressor: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Accuracy for XGBoost:\n",
      "98.88268156424581\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 6120, number of negative: 6120\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1459\n",
      "[LightGBM] [Info] Number of data points in the train set: 12240, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Accuracy for LightGBM:\n",
      "98.933468765871\n",
      "\n",
      "Training CatBoost...\n",
      "\n",
      "Accuracy for CatBoost:\n",
      "99.13661757237176\n",
      "\n",
      "Training NGBoost...\n",
      "Error with NGBoost: arrays used as indices must be of integer (or boolean) type\n",
      "\n",
      "Training River Random Forest...\n",
      "Error with River Random Forest: 'numpy.ndarray' object has no attribute 'iloc'\n",
      "\n",
      "Training Deep Neural Network...\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Accuracy for Deep Neural Network:\n",
      "98.62874555611985\n",
      "\n",
      "Accuracy Rankings (WITH StandardScaler):\n",
      "--------------------------------------------------\n",
      "Random Forest                  : 99.14%\n",
      "Extra Trees                    : 99.14%\n",
      "CatBoost                       : 99.14%\n",
      "Radius Nearest Neighbors       : 98.98%\n",
      "Bagging Classifier             : 98.98%\n",
      "Multi-Layer Perceptron         : 98.93%\n",
      "LightGBM                       : 98.93%\n",
      "Hist Gradient Boosting         : 98.88%\n",
      "SVM (Polynomial Kernel)        : 98.88%\n",
      "Gaussian Process               : 98.88%\n",
      "XGBoost                        : 98.88%\n",
      "Gradient Boosting              : 98.83%\n",
      "Label Propagation              : 98.83%\n",
      "SVM (RBF Kernel)               : 98.78%\n",
      "Label Spreading                : 98.78%\n",
      "K-Nearest Neighbors            : 98.63%\n",
      "Deep Neural Network            : 98.63%\n",
      "AdaBoost                       : 98.48%\n",
      "Extra Tree                     : 98.43%\n",
      "Decision Tree                  : 98.22%\n",
      "Quadratic Discriminant Analysis : 97.77%\n",
      "Logistic Regression            : 97.11%\n",
      "Logistic Regression CV         : 97.11%\n",
      "Linear SVM                     : 97.00%\n",
      "Calibrated Classifier          : 96.95%\n",
      "SVM (Linear Kernel)            : 96.65%\n",
      "Ridge Classifier               : 96.60%\n",
      "Ridge Classifier CV            : 96.60%\n",
      "Linear Discriminant Analysis   : 96.60%\n",
      "SGD Classifier                 : 96.50%\n",
      "Gaussian Naive Bayes           : 95.89%\n",
      "Nu SVM                         : 94.77%\n",
      "Nearest Centroid               : 92.28%\n",
      "Passive Aggressive             : 91.16%\n",
      "Perceptron                     : 90.20%\n",
      "Bernoulli Naive Bayes          : 86.34%\n",
      "Dummy Classifier               : 78.31%\n",
      "Local Outlier Factor           : 18.74%\n",
      "Isolation Forest               : 17.78%\n",
      "One-Class SVM                  : 7.47%\n"
     ]
    }
   ],
   "source": [
    "run_comparison(x_tr_resample, y_tr_resample, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rendots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
