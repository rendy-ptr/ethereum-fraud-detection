{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../Data/X_test.csv')\n",
    "y_test = np.loadtxt(\"../Data/y_test.csv\", delimiter=\",\")\n",
    "\n",
    "test_models = [\"LGBM\", \"ETC\", \"SVM\", \"GNB\", \"CATBOOST\"]\n",
    "model_dir = \"../Models/Tuning\"\n",
    "\n",
    "minmax = joblib.load(\"../Models/Tuning/minmax.pkl\")\n",
    "pt = joblib.load(\"../Models/Tuning/powertransformer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax_scaled = minmax.transform(X_test)\n",
    "\n",
    "X_pt_scaled = pt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Models...\n",
      "\n",
      "Evaluating LGBM...\n",
      "Evaluating ETC...\n",
      "Evaluating SVM...\n",
      "Evaluating GNB...\n",
      "Evaluating CATBOOST...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "print(\"\\nEvaluating Models...\\n\")\n",
    "for model_name in test_models:\n",
    "    model_path = os.path.join(model_dir, f\"MODEL-{model_name}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model {model_name} not found, skipping...\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform prediction\n",
    "    if model_name == \"SVM\":\n",
    "        y_pred = model.predict(X_minmax_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_minmax_scaled)[:, 1]\n",
    "    elif model_name == \"GNB\":\n",
    "        y_pred = model.predict(X_pt_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_pt_scaled)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # End time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0) * 100\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0) * 100\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0) * 100\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) * 100\n",
    "    computation_time = end_time - start_time\n",
    "    \n",
    "    # Store results in a list\n",
    "    results.append([model_name, computation_time, accuracy, precision, recall, f1, roc_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Testing Results:\n",
      "================================================================================\n",
      "Model      Time (s)     Accuracy     Precision    Recall       F1 Score     ROC AUC     \n",
      "================================================================================\n",
      "LGBM       0.0040       99.44        99.06        98.36        98.71        99.95       \n",
      "ETC        0.1100       99.49        99.29        98.36        98.82        99.94       \n",
      "SVM        0.1063       98.43        95.62        97.19        96.40        98.79       \n",
      "GNB        0.0010       95.58        85.56        95.78        90.39        98.18       \n",
      "CATBOOST   0.0030       99.44        99.06        98.36        98.71        99.95       \n",
      "================================================================================\n",
      "\n",
      "All models evaluated successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Testing Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<10} {'Time (s)':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1 Score':<12} {'ROC AUC':<12}\")\n",
    "print(\"=\" * 80)\n",
    "for res in results:\n",
    "    print(f\"{res[0]:<10} {res[1]:<12.4f} {res[2]:<12.2f} {res[3]:<12.2f} {res[4]:<12.2f} {res[5]:<12.2f} {res[6]:<12.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAll models evaluated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rendots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
